import googleapiclient.discovery
import pandas as pd

api_service_name = "youtube"
api_version = "v3"
DEVELOPER_KEY = "your api key"

youtube = googleapiclient.discovery.build(
    api_service_name, api_version, developerKey=DEVELOPER_KEY)

# List of video IDs
video_ids = ["BoEwSMcOHe0", "oZyVl9QUTIA", "AMcv8CjDzJc", "dd1i-Vorwlk"]

# List of specified emojis
specified_emojis = ['ğŸ¥º', ':(', 'ğŸ˜­', 'ğŸ˜¢', 'ğŸ˜”', 'ğŸ™', 'ğŸ˜“', 'ğŸ˜–', 'ğŸ˜', 'ğŸ˜£', 'ğŸ˜¥', 'ğŸ˜©', 'ğŸ˜«', 'ğŸ˜°', 'ğŸ’”', 'ğŸ˜•', 'ğŸ˜Ÿ', 'ğŸ˜¦', 'ğŸ˜§', 'ğŸ‘']

comments = []

for video_id in video_ids:
    request = youtube.commentThreads().list(
        part="snippet",
        videoId=video_id,
        maxResults=100
    )

    # Execute the request.
    response = request.execute()

    # Get the comments from the response.
    while True:
        for item in response['items']:
            comment = item['snippet']['topLevelComment']['snippet']
            text = comment['textOriginal']
            public = item['snippet']['isPublic']

            # Check if the comment contains any specified emoji
            if any(emoji in text for emoji in specified_emojis):
                comments.append([
                    2,  # Add ID to the row
                    text,
                ])

        try:
            nextPageToken = response['nextPageToken']
        except KeyError:
            break

        # Create a new request object with the next page token.
        nextRequest = youtube.commentThreads().list(part="snippet", videoId=video_id, maxResults=100,
                                                    pageToken=nextPageToken)
        # Execute the next request.
        response = nextRequest.execute()

# Create DataFrame with added 'Video_ID' column
df = pd.DataFrame(comments, columns=['ID', 'comment'])

# Write DataFrame to Excel file
df.to_excel('DB_sad.xlsx', index=False)
